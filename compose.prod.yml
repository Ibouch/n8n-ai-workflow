# ==============================================================================
# PRODUCTION ENVIRONMENT OVERRIDES
# ==============================================================================
# This file provides production-specific configurations and full observability stack.
# 
# USAGE REQUIREMENTS:
# - This file MUST be used with the base compose.yml file
# - Command: docker compose -f compose.yml -f compose.prod.yml up -d
# - Order matters: base file first, then production overrides
# 
# MAINTENANCE NOTES:
# - Shared anchors must be kept identical to compose.yml
# - Complete monitoring stack with production-grade resource limits
# - Independent secrets section for standalone deployment capability

# ==============================================================================
# PRODUCTION ENVIRONMENT ANCHORS
# ==============================================================================
# Monitoring service resource templates
x-monitoring-small-resources: &monitoring-small-resources
  limits:
    cpus: '0.5'
    memory: 512M
  reservations:
    cpus: '0.1'
    memory: 128M

x-monitoring-medium-resources: &monitoring-medium-resources
  limits:
    cpus: '1.5'
    memory: 2G
  reservations:
    cpus: '0.5'
    memory: 512M

x-monitoring-large-resources: &monitoring-large-resources
  limits:
    cpus: '2'
    memory: 4G
  reservations:
    cpus: '1'
    memory: 1G

x-monitoring-tmpfs: &monitoring-tmpfs
  - /tmp:noexec,nosuid,size=100m

services:
  # ==============================================================================
  # PRODUCTION DATABASE OPTIMIZATIONS
  # ==============================================================================
  
  postgres:
    environment:
      POSTGRES_INITDB_ARGS: "--auth-host=scram-sha-256 --auth-local=scram-sha-256 --data-checksums"
    command: >
      postgres
      -c shared_buffers=512MB
      -c effective_cache_size=2GB
      -c maintenance_work_mem=128MB
      -c checkpoint_completion_target=0.9
      -c wal_buffers=32MB
      -c default_statistics_target=100
      -c random_page_cost=1.1
      -c effective_io_concurrency=200
      -c work_mem=8MB
      -c min_wal_size=2GB
      -c max_wal_size=8GB
      -c max_worker_processes=8
      -c max_parallel_workers_per_gather=4
      -c max_parallel_workers=8
      -c max_parallel_maintenance_workers=4
      -c log_statement=mod
      -c log_connections=on
      -c log_disconnections=on
      -c log_duration=on
      -c log_min_duration_statement=500
      -c shared_preload_libraries='pg_stat_statements'
      -c log_lock_waits=on
      -c deadlock_timeout=1s
      -c log_checkpoints=on
    deploy:
      restart_policy:
        condition: on-failure
        delay: 10s
        max_attempts: 3
        window: 300s

  # ==============================================================================
  # PRODUCTION APPLICATION OPTIMIZATIONS
  # ==============================================================================
  
  n8n:
    environment:
      NODE_ENV: "production"
      NODE_OPTIONS: "--max-old-space-size=6144 --max-semi-space-size=128"
      
      EXECUTIONS_DATA_PRUNE: "true"
      EXECUTIONS_DATA_MAX_AGE: "168"
      EXECUTIONS_DATA_PRUNE_TIMEOUT: "3600"
      EXECUTIONS_DATA_PRUNE_HARD_DELETE_INTERVAL: "15"
      
      N8N_DISABLE_PRODUCTION_MAIN_PROCESS: "false"
      N8N_SKIP_WEBHOOK_DEREGISTRATION_SHUTDOWN: "false"
      N8N_BLOCK_FILE_ACCESS_TO_N8N_FILES: "true"
      N8N_DEFAULT_BINARY_DATA_MODE: "filesystem"
      N8N_BINARY_DATA_TTL: "2880"
      
      N8N_METRICS_PREFIX: "n8n_"
      N8N_QUEUE_HEALTH_CHECK_ACTIVE: "true"
    deploy:
      restart_policy:
        condition: on-failure
        delay: 15s
        max_attempts: 5
        window: 300s
      update_config:
        parallelism: 1
        delay: 30s
        failure_action: rollback
        monitor: 120s
        max_failure_ratio: 0.2
      resources:
        limits:
          cpus: '6'
          memory: 8G
        reservations:
          cpus: '2'
          memory: 4G

  # ==============================================================================
  # PRODUCTION REVERSE PROXY OPTIMIZATIONS
  # ==============================================================================
  
  nginx:
    sysctls:
      - net.core.somaxconn=65535
      - net.ipv4.ip_local_port_range=1024 65535
      - net.core.netdev_max_backlog=5000
    ulimits:
      nofile:
        soft: 65536
        hard: 65536
      nproc:
        soft: 65536
        hard: 65536
    deploy:
      restart_policy:
        condition: on-failure
        delay: 10s
        max_attempts: 3
        window: 180s
      resources:
        limits:
          cpus: '2'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 512M

  # ==============================================================================
  # MONITORING & OBSERVABILITY STACK
  # ==============================================================================
  
  prometheus:
    image: prom/prometheus:latest
    container_name: n8n-prometheus
    restart: unless-stopped
    user: "65534:65534"
    read_only: true
    security_opt:
      - no-new-privileges:true
      - seccomp:./security/seccomp-profile.json
    cap_drop:
      - ALL
    tmpfs: *monitoring-tmpfs
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
      - '--storage.tsdb.retention.time=45d'
      - '--storage.tsdb.retention.size=10GB'
      - '--web.enable-lifecycle'
      - '--web.enable-admin-api'
      - '--storage.tsdb.wal-compression'
    volumes:
      - ./monitoring/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./monitoring/prometheus/alerts.yml:/etc/prometheus/alerts.yml:ro
      - prometheus_data:/prometheus
    networks:
      - n8n-backend
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:9090/-/ready || exit 1"]
      interval: 60s
      timeout: 15s
      retries: 3
      start_period: 60s
    deploy:
      resources: *monitoring-large-resources

  grafana:
    image: grafana/grafana:latest
    container_name: n8n-grafana
    restart: unless-stopped
    user: "472:472"
    read_only: true
    security_opt:
      - no-new-privileges:true
      - seccomp:./security/seccomp-profile.json
    cap_drop:
      - ALL
    tmpfs:
      - /tmp:noexec,nosuid,size=100m
      - /var/lib/grafana/plugins:noexec,nosuid,size=200m
      - /etc/grafana:noexec,nosuid,size=10m
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_ADMIN_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD_FILE=/run/secrets/grafana_password
      - GF_INSTALL_PLUGINS=grafana-clock-panel,grafana-piechart-panel,redis-datasource
      - GF_SERVER_ROOT_URL=${GRAFANA_ROOT_URL:-http://localhost:3000}
      - GF_ANALYTICS_REPORTING_ENABLED=false
      - GF_ANALYTICS_CHECK_FOR_UPDATES=false
      - GF_SECURITY_DISABLE_GRAVATAR=true
      - GF_USERS_DEFAULT_THEME=dark
      - GF_FEATURE_TOGGLES_ENABLE=publicDashboards
    secrets:
      - grafana_password
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning:ro
    networks:
      - n8n-backend
      - n8n-frontend
    depends_on:
      - prometheus
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:3000/api/health || exit 1"]
      interval: 60s
      timeout: 15s
      retries: 3
      start_period: 60s
    deploy:
      resources: *monitoring-medium-resources

  loki:
    image: grafana/loki:latest
    container_name: n8n-loki
    restart: unless-stopped
    user: "10001:10001"
    read_only: true
    security_opt:
      - no-new-privileges:true
      - seccomp:./security/seccomp-profile.json
    cap_drop:
      - ALL
    tmpfs: *monitoring-tmpfs
    command: -config.file=/etc/loki/local-config.yaml
    volumes:
      - ./monitoring/loki/loki-config.yaml:/etc/loki/local-config.yaml:ro
      - loki_data:/loki
    networks:
      - n8n-backend
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:3100/ready || exit 1"]
      interval: 60s
      timeout: 15s
      retries: 3
      start_period: 60s
    deploy:
      resources: *monitoring-medium-resources

  promtail:
    image: grafana/promtail:latest
    container_name: n8n-promtail
    restart: unless-stopped
    user: "0:0"
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - DAC_OVERRIDE
    tmpfs: *monitoring-tmpfs
    volumes:
      - /var/log:/var/log:ro
      - /var/lib/docker/containers:/var/lib/docker/containers:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - ./monitoring/promtail/promtail-config.yaml:/etc/promtail/config.yml:ro
    networks:
      - n8n-backend
    command: -config.file=/etc/promtail/config.yml
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:9080/ready || exit 1"]
      interval: 60s
      timeout: 15s
      retries: 3
      start_period: 60s
    deploy:
      resources: *monitoring-small-resources

  alertmanager:
    image: prom/alertmanager:latest
    container_name: n8n-alertmanager
    restart: unless-stopped
    user: "65534:65534"
    read_only: true
    security_opt:
      - no-new-privileges:true
      - seccomp:./security/seccomp-profile.json
    cap_drop:
      - ALL
    tmpfs: *monitoring-tmpfs
    environment:
      - SMTP_HOST=${SMTP_HOST:-localhost}
      - SMTP_PORT=${SMTP_PORT:-587}
      - SMTP_FROM=${SMTP_FROM:-alertmanager@localhost}
      - SMTP_USERNAME=${SMTP_USERNAME:-alertmanager@localhost}
      - SMTP_TLS=${SMTP_TLS:-true}
      - ALERT_EMAIL_TO=${ALERT_EMAIL_TO:-admin@localhost}
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/alertmanager'
      - '--web.external-url=http://localhost:9093'
      - '--web.route-prefix=/'
      - '--data.retention=168h'
    secrets:
      - smtp_password
    volumes:
      - ./monitoring/alertmanager/alertmanager.yml:/etc/alertmanager/alertmanager.yml:ro
      - alertmanager_data:/alertmanager
    networks:
      - n8n-backend
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:9093/-/ready || exit 1"]
      interval: 60s
      timeout: 15s
      retries: 3
      start_period: 60s
    deploy:
      resources: *monitoring-small-resources

  postgres-exporter:
    image: prometheuscommunity/postgres-exporter:latest
    container_name: n8n-postgres-exporter
    restart: unless-stopped
    user: "65534:65534"
    read_only: true
    security_opt:
      - no-new-privileges:true
      - seccomp:./security/seccomp-profile.json
    cap_drop:
      - ALL
    tmpfs: *monitoring-tmpfs
    environment:
      - DATA_SOURCE_URI=postgres:5432/${POSTGRES_DB:-n8n}?sslmode=disable
      - DATA_SOURCE_USER=${POSTGRES_USER:-n8n_admin}
      - DATA_SOURCE_PASS_FILE=/run/secrets/postgres_password
      - PG_EXPORTER_EXTEND_QUERY_PATH=/etc/postgres_exporter/queries.yaml
    secrets:
      - postgres_password
    volumes:
      - ./monitoring/postgres-exporter:/etc/postgres_exporter:ro
    networks:
      - n8n-backend
    depends_on:
      postgres:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:9187/metrics || exit 1"]
      interval: 60s
      timeout: 15s
      retries: 3
      start_period: 60s
    deploy:
      resources: *monitoring-small-resources

  redis-exporter:
    image: oliver006/redis_exporter:latest
    container_name: n8n-redis-exporter
    restart: unless-stopped
    user: "59000:59000"
    read_only: true
    security_opt:
      - no-new-privileges:true
      - seccomp:./security/seccomp-profile.json
    cap_drop:
      - ALL
    tmpfs: *monitoring-tmpfs
    environment:
      - REDIS_ADDR=redis://redis:6379
      - REDIS_PASSWORD_FILE=/run/secrets/redis_password
      - REDIS_EXPORTER_LOG_FORMAT=json
    secrets:
      - redis_password
    networks:
      - n8n-backend
    depends_on:
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:9121/metrics || exit 1"]
      interval: 60s
      timeout: 15s
      retries: 3
      start_period: 60s
    deploy:
      resources: *monitoring-small-resources

  node-exporter:
    image: prom/node-exporter:latest
    container_name: n8n-node-exporter
    restart: unless-stopped
    user: "65534:65534"
    read_only: true
    security_opt:
      - no-new-privileges:true
      - seccomp:./security/seccomp-profile.json
    cap_drop:
      - ALL
    tmpfs: *monitoring-tmpfs
    command:
      - '--path.procfs=/host/proc'
      - '--path.rootfs=/rootfs'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
      - '--collector.netdev.device-exclude=^(veth.*|docker.*|br-.*|lo)$'
      - '--collector.diskstats.ignored-devices=^(ram|loop|fd|(h|s|v|xv)d[a-z]|nvme\\d+n\\d+p)\\d+$'
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    networks:
      - n8n-backend
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:9100/metrics || exit 1"]
      interval: 60s
      timeout: 15s
      retries: 3
      start_period: 60s
    deploy:
      resources: *monitoring-small-resources

  cadvisor:
    image: gcr.io/cadvisor/cadvisor:latest
    container_name: n8n-cadvisor
    restart: unless-stopped
    read_only: true
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - SYS_ADMIN
    tmpfs: *monitoring-tmpfs
    command:
      - '--housekeeping_interval=30s'
      - '--docker_only=true'
      - '--store_container_labels=false'
      - '--whitelisted_container_labels=io.kubernetes.container.name,io.kubernetes.pod.name,io.kubernetes.pod.namespace'
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
      - /dev/disk/:/dev/disk:ro
    networks:
      - n8n-backend
    privileged: false
    devices:
      - /dev/kmsg
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:8080/healthz || exit 1"]
      interval: 60s
      timeout: 15s
      retries: 3
      start_period: 60s
    deploy:
      resources: *monitoring-small-resources

volumes:
  prometheus_data:
    driver: local
  grafana_data:
    driver: local
  loki_data:
    driver: local
  alertmanager_data:
    driver: local

secrets:
  postgres_password:
    file: ./secrets/postgres_password.txt
  
  n8n_password:
    file: ./secrets/n8n_password.txt
  n8n_encryption_key:
    file: ./secrets/n8n_encryption_key.txt
  
  redis_password:
    file: ./secrets/redis_password.txt
  
  smtp_password:
    file: ./secrets/smtp_password.txt
  
  grafana_password:
    file: ./secrets/grafana_password.txt